# Experiment 
seed_start: 2021 # Starting seed
num_seeds: 5 # Number of seeds

name_dataset: sigmoid_time100_train20_valid100 # Load if exists, otherwise create
num_steps: 100 # Number of timesteps in offline data
train_per_step: 10 # N umber of training datapoints per step
valid_per_step: 100 # number of validation datapoints per step
size_batch: 100 # Batch size
drift_coef: 1.0 # if 1.0, the parameter becomes orthogonal to the original at the end of the drift
sample_prefs: True # Used in data generation, if False draws from binomial distribution

freq_report: 5 # How often to report frequency
logdir: logs/

# environment
# Creates bandit environment. If OfflineBandit, train_offline in Agent.py is called
env_bandit: LinearBandit # LinearBandit, OfflineBandit
state_dim: 4 # State dimension, ie number of features
action_num: 16 # Number of actions 
reset_state_dist: uniform # Controls state distribution reset (normal or uniform)

# Set initial reward 
reward_v1: 0.018215
reward_v2: 0.11031

# time-varying setting
# Controls the reward parameters and how they change with time
tv:
  use: True # If true uses time varying reward for online/offline data
  type: faury # linear or faury
  # Set reward at end of time varying sim
  v1_target: 0.11031 
  v2_target: 0.018215

# agent
algo: dpo
tv_dpo: False
use_sw: False

# Agent parameters
max_param_norm: 1. 
step_size: 0.1
num_iters: 100
freq_eval: 100
is_adaptive: True
ada_coef: 0.1
gamma: 1.0

# NS-DPO parameters
reg_coef: 1.0
gamma2: None

# sigmoidloss-particular
cs: 1.
ks: 1.
l2_coef: 0.01

# Sliding-Window DPO
window_size: 30

# action selection
action_selection: random
filter_actions: False
cov_init_coef: 1.
bonus_coef: 1.0

# Weights and biases
wandb:
  use: false
  key: YOUR_KEY
  entity: YOUR_ENTITY
  project: project0
  group: group0
