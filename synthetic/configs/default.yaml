# Experiment 
seed_start: 2021 # Starting seed
num_seeds: 1 # Number of seeds

num_data: 20 # Number of timesteps in simulation, used only in Linear Bandit
freq_report: 5 # How often to report frequency
logdir: logs/

wandb:
  use: false
  key: YOUR_KEY
  entity: YOUR_ENTITY
  project: project0
  group: group0

# environment
# Creates bandit environment. If OfflineBandit, train_offline in Agent.py is called
env_bandit: LinearBandit # LinearBandit, OfflineBandit
state_dim: 4 # State dimension, ie number of features
action_num: 16 # Number of actions 
reset_state_dist: uniform # Controls state distribution reset (normal or uniform)

# Set initial reward 
reward_v1: 0.018215
reward_v2: 0.11031

# time-varying setting
# Controls the reward parameters and how they change with time
tv:
  use: False # If true uses time varying reward for online/offline data
  type: linear # linear or faury
  # Set reward at end of time varying sim
  v1_target: 0.11031 
  v2_target: 0.018215

# offline dataset
odata:
  name_dataset: sigmoid_time100_train20_valid100 # Load if exists, otherwise create
  num_steps: 100 # Number of timesteps in offline data
  train_per_step: 20 # N umber of training datapoints per step
  valid_per_step: 100 # number of validation datapoints per step
  size_batch: 100 # Batch size
  drift_coef: 1.0 # if 1.0, the parameter becomes orthogonal to the original at the end of the drift
  sample_prefs: True # Used in data generation, if False draws from binomial distribution

# agent
algo: dpo
max_param_norm: 1. 
max_feature_norm: 4.
step_size: 0.1
num_iters: 200
freq_eval: 20
reg_coef: 1.0
l2_coef: 0.01
is_adaptive: True
ada_coef: 0.1
delta: 0.1 # for performance guarantees

# sigmoidloss-particular
cs: 1.
ks: 1.
gamma: 1.
gamma2: None

# sliding-window dpo
use_sw: False
window_size: 50

# action selection
action_selection: random
filter_actions: False
cov_init_coef: 1.
bonus_coef: 1.0
